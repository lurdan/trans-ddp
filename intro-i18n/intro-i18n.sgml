<!doctype debiandoc public "-//DebianDoc//DTD DebianDoc//EN"
[
   <!entity % languages system "languages.ents"> %languages;
   <!entity % examples system "examples.ents"> %examples;
]>
<debiandoc>
<book>


<titlepag>
  <title>Introduction to i18n</title>
  <author>
    <name>Tomohiro KUBOTA</name>
    <email>kubota@debian.or.jp</email>
  </author>
  <version><date></version>
  <abstract>
    This document describes introduction to i18n (internationalization)
    for programmers and package maintainers.
  </abstract>
  <copyright>
    <copyrightsummary>
      Copyright &copy; 1999-2000 Tomohiro KUBOTA.
      For chapters and sections whose original author is not KUBOTA,
      the authors of them have copyright.  Their names are written
      at the top of the chapter or the section.
    </copyrightsummary>
    <p>
      This manual is free software; you may redistribute it and/or modify it
      under the terms of the GNU General Public License as published by the
      Free Software Foundation; either version 2, or (at your option) any
      later version.
    </p>
    <p>
      This is distributed in the hope that it will be useful, but
      <em>without any warranty</em>; without even the implied warranty of
      merchantability or fitness for a particular purpose.  See the GNU
      General Public License for more details.
    </p>
    <p>
      A copy of the GNU General Public License is available as
      <tt>/usr/share/common-licenses/GPL</tt> in the Debian GNU/Linux 
      distribution or on the World Wide Web at 
      <url id="http://www.gnu.org/copyleft/gpl.html">.
      You can also obtain it by writing to the Free
      Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
      02111-1307, USA.
    </p>
  </copyright>
</titlepag>

<toc detail="sect1">

<chapt id="scope"><heading>About This Document</heading>

<sect id="scope2"><heading>Scope</heading>

<P>
This document describes the basic ideas of I18N written for
programmers and package maintainers of Debian GNU/Linux and
other UNIX-like platforms.
The aim of this document is to offer an introduction to 
basic concepts, character codes, and points of which care 
should be taken when one writes an I18N-ed software or
an I18N patch for an existing software.  This document
also tries to introduce the real state and existing
problems for each language and country.
</P>

<P>
Minimum requirements, for example, 
that characters should be displayed proper font (at least users
of the software must be able to guess what is written), 
that characters must be inputed from keyboard, and
that softwares must not destroy characters, 
are stressed in the document and I am trying to 
describe a HOWTO to satisfy these requirements.
</P>

<P>
This document is strongly related to programming 
languages such as C and standardized I18N methods such as
LOCALE and <prgn>gettext</prgn>.
</P>

<sect id="newversion"><heading>New Versions of This Document</heading>

<P>
The current version of this document is available
at 
<url id="http://www.debian.org/~elphick/ddp/" 
name="DDP (Debian Documentation Project)"> page.
</P>

<p>
Note that the author rewrite this document in November 2000.
</p>

<sect id="feedback"><heading>Feedback and Contributions</heading>

<P>
This document needs contributions, especially for a 
chapter on each languages (<ref id="languages">)
and a chapter on instances of I18N (<ref id="examples">). 
These chapters are consist of contributions.
</P>

<P>
Otherwise, this will be a mere document only on Japanization, 
because the original author Tomohiro KUBOTA 
(<email>kubota@debian.or.jp</email>) 
speaks Japanese and live in Japan.
</P>

<P>
<ref id="spanish"> is written by 
Eusebio C Rufian-Zilbermann <email>eusebio@acm.org</email>.
</P>

<P>
Discussions are held at <tt>debian-devel@lists.debian.org</tt> mailing list.
(May <tt>debian-doc</tt> or <tt>debian-i18n</tt> be more suitable?)
</P>

<chapt id="intro"><heading>Introduction</heading>

<sect id="intro-concepts"><heading>General Concepts</heading>

<P>
Debian system includes many softwares.  Though many of them
have faculty to process, output, and input text data, a part
of these programs assume text as written in English (ASCII).
For people who use non-English language these programs are
hardly usable.  And more, though many softwares which can handle
not only ASCII but also ISO-8859-1, a certain amount of them
cannot handle multibyte characters for CJK (Chinese, Japanese,
and Korean) languages nor combined characters for Thai and so on.
</P>

<P>
So far people who use non-English languages have given up
to use their native languages and have accepted computers as such.
However we should throw away such a wrong idea now.  
It is nonsense that a person who
want to use a computer has to learn English in advance.
</P>

<P>
I18N is needed for the following places.
<list>
 <item>Display characters for users' native languages.
 <item>Input characters for users' native languages.
 <item>Handle files written in popular character codes
       <footnote>
        There are a few terms related to character code,
	such as character set, character code, charset,
	encoding, codeset, and so on.  These words are explained
	later.
       </footnote>
       used for users' native languages.
 <item>Use characters for users' native languages for file names
       and so on.
 <item>Print out characters for users' native languages.
 <item>Display messages by the softwares in users' native languages.
 <item>Formats for input/output of numbers, dates, money, and so on
       obey customs in users' native cultures.
 <item>Classification and sorting rules of characters obey customs
       in users' native cultures.
 <item>Use typesetting and hyphenation rules for the users' native
       languages.
 <item>and so on.
</list>
This document puts stress on the first three items.  This is because
these three items are the basis for the other items.  An another
reason is that:  you cannot use softwares lacking the first three
items at all, though you can use softwares lacking the other items
inconveniently.  This document will also mention translation of
messages (item 6) which is often called as 'I18N'.  Note that
the author regards the terminology of 'I18N' for calling translation
and <prgn>gettext</prgn>ization is completely wrong.  The reason
may be well explained by the fact that the author did not include
translation and <prgn>gettext</prgn>ization in the important first
three items.
</P>

<P>
Imagine a word processor which can display error
and help messages in your native language while cannot process
your native language.  You will easily understand that the word
processor is not usable.  On the other hand, a word processor which
can process your native language while error and help messages are
displayed only in English is usable, though it is not convenient.
Before we think of developing convenient softwares, we have to
think of developing usable softwares.
</P>

<P>
The following terminology is widely used.
<list>
 <item>I18N (internationalization) means modification of a software
       or related technologies so that a software can potentially
       handle multiple languages, customs, and so on in the world.
 <item>L10N (localization) means implementation of a specific language
       for an already internationalized software.
</list>
However, this terminology is valid only for one specific model
out of a few models which we should regard for I18N.
Now I will introduce a few models other than this I18N-L10N model.
<taglist>
 <tag>a. <strong>L10N</strong> (localization)-model</tag>
   <item><p>
	This model is to support two languages or character codes,
	English (ASCII) and another specific one.  Examples of
	softwares which is developed using this model are:
	Nemacs (Nihongo Emacs, an ancestor of MULE, MULtilingual
	Emacs) text editor which can input/output Japanese text file,
	Hanterm X terminal emulator which can display and input
	Korean characters via a few Korean character codes.
	Since a programmer has his/her own mother tongue,
	there are numerous L10N patches and L10N softwares 
	written to satisfy his/her own need.
   </p></item>
 <tag>b. <strong>I18N</strong> (internationalization)-model</tag>
   <item><p>
	This model is to support many languages but only two 
	of them, English (ASCII) and another one, at the same time. 
	One have to specify the 'another' language by <tt>LANG</tt> 
	environmental variable or so on.
	The above I18N-L10N-model can be regarded as a part of
	this I18N-model.
	<prgn>gettext</prgn>ization is categorized into I18N-model.
   </p></item>
 <tag>c. <strong>M17N</strong> (multilingualization)-model</tag>
   <item><p>
	This model is to support many languages at the same time.
	For example, Mule (MULtilingual Enhancement to GNU Emacs) 
	can treat a text file which contains multiple languages,
	for example, a paper on difference between Korean and Chinese
	whose main text is written in Finnish.  Now GNU Emacs 20 and 
	XEmacs include Mule.
	Note that M17N-model can only be applied to character-related
	'places'.  For example, it is nonsense to display a message
	like 'file not found' in many languages at the same time.
	Unicode and UTF-8 are technologies which can be used for
	this model.
	<footnote>
	  I recommend not to implement Unicode and UTF-8 directly.
	  Instead, use LOCALE technology and your software will
	  support not only UTF-8 but also many character codes 
	  in the world.
	</footnote>
   </p></item>
</taglist>
</P>

<P>
Generally speaking, M17N-model is the best and the next is 
I18N-model.  L10N-model is the worst and you should not take it 
except for a few fields where I18N and M17N-models are very difficult, 
like DTP and X terminal emulator.
In other words, text-processing softwares are 'better' which can treat
many languages at the same time, than can treat two (English and an 
another) languages.
</P>

<P>
Now let me classify approaches for support of non-English languages
from an another viewpoint.
<taglist>
 <tag>A. Implementation <em>without</em> Knowledge on Each Language</tag>
   <item><p>
	This approach is done by utilizing standardized methods supplied 
	by the kernel or libraries.  The most important one is 
	<strong>locale</strong> technology which includes 
	<strong>locale category</strong>, conversion between 
	<strong>multibyte</strong> and <strong>wide
	characters</strong> (<tt>wchar_t</tt>), and so on.
	Another important technology is <prgn>gettext</prgn>.
	The advantages of this approach are (1) that when the kernel or
	libraries are upgraded, the software will automatically
	support new additional languages, (2) that programmers need
	not know each language, and (3) that a user can switch the behavior
	of softwares with common method, like LANG variable.
	The disadvantage is that there are categories or fields where
	a standardized method is not available.  For example, there
	are no standardized methods for text typesetting rules such
	as line-breaking and hyphenation.
   </p></item>
 <tag>B. Implementation Using Knowledge on Each Language</tag>
   <item><p>
	This approach is to directly implement information about 
	each language based on knowledge of programmers and 
	contributors.  L10N almost always uses this approach.
	The advantage of this approach is that detailed and strict
	implementation is possible beyond the field where
	standardized methods are available, such as auto-detection
	of character codes of text files to be read.  Language-specific
	problems can be perfectly solved (of course it depends on
	the skill of the programmer).  The disadvantages are
	(1) that the number of supported languages is restricted
	by the skill or the interest of the programmers or the
	contributors, (2) that labor which should be united and
	concentrated to upgrade the kernel or libraries is dispersed
	into many softwares, that is, re-inventing of the wheel,
	and (3) a user has to learn how to configure each software,
	such as <tt>LESSCHARSET</tt> variable, <tt>.emacs</tt> file, 
	and so on so on.
	This approach can cause problems: for example, GNU roff
	(before version 1.16) assumes <tt>0xad</tt> as a hyphen
	character, which is valid only for ISO-8859-1.
	However a majestic M17N software such as Mule can be
	built by strongly propel this approach.
   </p></item>
</taglist>
</P>

<P>
Using this classification, let me consider L10N, I18N, and M17N-models
from programmer's point of view.
</P>

<P>
L10N-model can be realized only using his/her own knowledge on his/her
language (i.e. approach B).  Since the motivation of L10N is
usually to satisfy programmer's own need, extensiveness for the
third languages is often ignored.
Though L10N-ed softwares are basically useful for people who 
speaks the same language to the programmer, it is sometimes 
useful for other people whose coding system is similar to 
the programmer's.  For example, a software which 
doesn't recognize EUC-JP but doesn't break EUC-JP, will not 
break EUC-KR also. 
</P>

<P>
Main part of I18N-model is, in the case of C program, achieved using
standardized LOCALE technology and <prgn>gettext</prgn>.
An LOCALE approach is classified into I18N because functions
related to LOCALE change their behavior by the current locales
for six categories which are set by <tt>setlocale()</tt>.
Namely, approach A is emphasized for I18N. For field where
standardized methods are not available, however, approach B
cannot be avoided.  Even in such a case, the developers should
be careful so that a support for new languages can be easily added
later even by other developers.
</P>

<P>
M17N-model can be achieved using international character codes such
as ISO-2022 and Unicode.  Though you can hard-code these character codes
for your software (i.e. approach B), I recommend to use standardized
LOCALE technology.  However, using international character codes
is not sufficient to achieve M17N-model.  You will have to prepare
a mechanism to switch <strong>input methods</strong>.  You will also want
to prepare a character code-guessing mechanism for input files.
Mule is the only software which achieved M17N (though it does not
use LOCALE technology).
</P>

<sect id="intro-organization"><heading>Organization</heading>

<P>
Let's preview the contents of each chapter in this document.
</P>

<P>
I have already wrote that this document will put stress on
correct handling of characters and character codes for users' native
languages.  To achieve this purpose, I will discuss on popular 
character codes in the world at first.  You will not need the detailed 
knowledges for these character codes if you will use LOCALE technology.
The chapter is only for showing the concepts used in these character
codes.  Understanding these concepts will help you to understand the 
merit and necessity of LOCALE technology.
</P>

<P>
The following chapter describes the detailed informations for 
each language.  These informations will help people who develop
high-quality text processing softwares such as DTP and Web Browsers.
</P>

<P>
Direct knowledges for software development are discussed 
in the following several chapters: Output to Display, 
Input from Keyboard, and Internal Processing.  The next chapter
describes other important topics such as <prgn>gettext</prgn>
and data exchange via the Internet (Mail/News/WWW).
<footnote>
I may change the structure of these chapters in the future.
</footnote>
</P>

<P>
The last chapter is a collection of case studies.
Both of generic and special technologies will be discussed.
You may want to study more; References are supplied.
Some of them are very important.
</P>


<chapt id="coding"><heading>Character Coding Systems</heading>

<P>
Here major character codes are introduced.
Note that you don't have to know the detail of these
character codes if you use LOCALE and <tt>wchar_t</tt> technology.
However, these knowledge will help you to understand why number
of bytes, characters, and columns should be counted separately,
why <tt>strchr()</tt> and so on should not be used, why you should
use LOCALE and <tt>wchar_t</tt> technology instead of hard-code
processing of existing character codes, and so on so on.
</P>

<P>
If you are planning to develop a text-processing software
beyond the fields which the LOCALE technology covers, you will
have to understand the following descriptions very well.
These fields include automatic detection of character code
used for the input file (Most of Japanese-capable text viewers
such as <prgn>jless</prgn> and <prgn>lv</prgn> have this mechanism)
and so on.
</P>


<sect id="coding-general"><heading>General Discussions</heading>

<sect1 id="codeset"><heading>Basic Terminology</heading>

<P>
At first I begin this chapter by defining a few very important word.
</P>

<P>
 <taglist>
  <tag><strong>Character</strong>
    <item><p>
          Character is an individual unit of which sentence and text
          consist.  Character is an abstract notion.
    </p></item>
  <tag><strong>Glyph</strong>
    <item><p>
          Glyph is a specific instance of character.  <em>Character</em>
	  and <em>glyph</em> is a pair of words.  Sometimes a character
	  has multiple glyphs (for example, '$' may have one or two vertical
	  bar.  Arabic characters have four glyphs for each character.
	  Some of CJK ideograms have many glyphs).  Sometimes two or more
	  characters construct one glyph (for example, ligature of 'fi').
	  For almost cases, text data, which intend to contain not
	  visual information but abstract idea, don't have to have
	  information on glyphs, since difference between glyphs does
	  not affect the meaning of the text.  However, distinction
	  between different glyphs for a single CJK ideogram may be
	  sometimes important for proper noun such as names of
	  persons and places.  However, there are no standardized method
	  for plain text to have informations on glyphs so far.  This
	  makes plain texts cannot be used for some special fields
	  such as citizen registration system, serious DTP such as
	  newspaper system, and so on.
    </p></item>
  <tag><strong>Character Code</strong>
    <item><p>
          Character code is a rule where characters and texts are
	  expressed in combinations of bits or bytes in order to
	  treat characters in computers.  Words of <em>character 
	  coding system</em>, <em>charset</em>, and so on are used
	  to express the same meaning.  Basically, character code 
	  takes care of <em>characters</em>, not <em>glyphs</em>.
	  There are many official and de-facto standards of character
	  codes such as ASCII, ISO 8859-{1,2,...,15}, 
	  ISO 2022-{JP, JP-1, JP-2, KR, CN, CN-EXT, INT-1, INT-2}, 
	  EUC-{JP, KR, CN, TW}, Johab, UHC, Shift-JIS, Big5, TIS 620, 
	  VISCII, VSCII, so-called 'CodePages', UTF-7, UTF-8, UTF-16LE, 
	  UTF-16BE, KOI8-R, and so on so on.
	  To construct a character code, we have to consider the
	  following concepts.  (Character code = one or more 
	  CCS + one CES).
    </p></item>
  <tag><strong>Character Set</strong>
    <item><p>
          Character set is a set of characters.  This determines
	  a range of characters where the character code can handle.
    </p></item>
  <tag><strong>Coded Character Set (CCS)</strong>
    <item><p>
          Coded character set (CCS) is a word defined in RFC 2050 
	  and means a character set where all characters
	  have unique numbers by some method.  There are many national
	  and international standards for CCS.
	  Many national standards for CCS adopt
	  the way of coding so that they obey some of international
	  standards such as ISO 646 or ISO 2022.  ASCII, BS 4730,
	  JISX 0201 Roman, and so on are examples of ISO-646 variants.  All 
	  ISO-646 variants, ISO 8859-*, JISX 0208, JISX 0212, KSC 1001,
	  GB 2312, CNS 11643, CCCII, TIS 620, TCVN 5712, and so on are 
	  examples of ISO 2022-compliant CCS.  VISCII and Big5 are 
	  examples of non-ISO 2022-compliant 
	  CCS.  UCS-2 and UCS-4 (ISO 10646) are also examples of CCS.
    </p></item>
  <tag><strong>Character Encoding Scheme (CES)</strong>
    <item><p>
          Character Encoding Scheme is also a word defined in RFC 2050
	  to call methods to construct a character code using one or 
	  more CCS.  This is important when two or more CCS are used 
	  to construct a character code.  
	  ISO 2022 is a method to construct a character code from
	  one or more ISO 2022-compliant CCS.  ISO 2022 is very
	  complex system and subsets of ISO 2022 are usually used
	  such as EUC-JP (ASCII and JISX 0208), ISO-2022-KR (ASCII
	  and KSX 1001), and so on.  CES is not important for 
	  character codes with only one CCS.
	  UTF series (UTF-8, UTF-16LE, UTF-16BE, and so on) can be
	  regarded as CES whose CCS is Unicode or ISO 10646.
    </p></item>
</taglist>
</P>


<sect1 id="stateful"><heading>Stateless and Stateful</heading>

<P>
To construct a character code with two or more CCS, 
CES has to supply a method to avoid collision between these CCS.
There are two ways to do that.  One is to make all characters
in the all CCS have unique code points.  The other is to
allow characters from different CCS to have the same
code point and to have a code such as escape sequence to switch
<strong>SHIFT STATE</strong>, that is, to select one character set.
</P>

<P>
A character code with shift state is called <strong>STATEFUL</strong> and
one without shift state is called <strong>STATELESS</strong>.
</P>

<P>
Examples of stateful character codes are: ISO 2022-*, 

<P>
For example, in ISO 2022-JP, two bytes of <tt>0x24 0x2c</tt> may mean
a Japanese Hiragana character 'GA' or two ASCII character of
'$' and ',' according to the shift state.
</P>

<sect1 id="multibyte"><heading>Multibyte character code</heading>

<P>
Character codes are classified into multibyte ones and the others,
according to the relationship between number of characters and number of
bytes in the character code.
</P>

<P>
In non-multibyte character code, one character is always expressed
by one byte.  On the other hand, one character may expressed in
one or more bytes in multibyte character code.  Note that the number
is not fixed even in a single character code.
</P>

<P>
Examples of multibyte character codes are: EUC-*, ISO 2022-*,
Shift-JIS, Big5, UTF-*, and so on.  Note that all of UTF-* are
multibyte.
</P>

<P>
Examples of non-multibyte character codes are: ISO 8859-*,
TIS 620, VISCII, and so on.
</P>

<P>
Note that even in non-multibyte character code, number of characters
and number of bytes may differ if the character code is stateful.
</P>

<sect1 id="number"><heading>Number of Bytes, Number of Characters, and Number of Columns</heading>

<P>
One ASCII character is always expressed by one byte
and occupies one column on console or X terminal emulators
(fixed font for X).
One must not make such an assumption for I18N programming
and have to clearly distinguish number of bytes, characters,
and columns.
</P>

<P>
Speaking of relationship between characters and bytes,
in multibyte character codes, two or more bytes may be needed
to express one character.  In stateful character codes, escape
sequences are not related to any characters.
</P>

<P>
Number of columns is not defined in any standards.  However,
it is usual that CJK ideograms, Japanese Hiragana and Katakana,
and Korean Hangul occupy two columns in console or X terminal emulators.
Note that 'Full-width forms' in UCS-2 and UCS-4 coded character set
will occupy two columns and 'Half-width forms' will occupy one column.
Combining characters used for Thai and so on can be regarded as
zero-column characters.
</P>

<sect id="standards"><heading>Standards for Character Codes</heading>

<sect1 id="ascii"><heading>ASCII and ISO 646</heading>

<P>
<strong>ASCII</strong> is a CCS and also a character code at the same time.
ASCII is 7bit and contains 94 printable characters which are 
encoded in the region of <tt>0x21</tt>-<tt>0x7e</tt>.  
</P>

<P>
<strong>ISO 646</strong> is the international standard of ASCII. 
Following 12 characters of
<list>
   <item>0x23 (number),
   <item>0x24 (dollar),
   <item>0x40 (at),
   <item>0x5b (left square bracket),
   <item>0x5c (backslash),
   <item>0x5d (right square bracket),
   <item>0x5e (caret),
   <item>0x60 (backquote),
   <item>0x7b (left curly brace),
   <item>0x7c (vertical line),
   <item>0x7d (right curly brace), and
   <item>0x7e (tilde)
</list>
are called <strong>IRV</strong> (International Reference Version) 
and other 82 (94 - 12 = 82) characters are called
<strong>BCT</strong> (Basic Code Table).
Characters at IRV can be different between countries.
Here is a few examples of versions of ISO 646.
<list>
  <item>UK version (BS 4730)
  <item>US version (ASCII): 0x23 is pound currency mark, and so on.
  <item>Japanese version (JISX 0201 Roman): 0x5c is yen currency mark, and
        so on.
  <item>Italian version (UNI 0204-70): 0x7b is 'a' with grave accent, and
        so on.
  <item>French version (NF Z 62-010): 0x7b is 'e' with acute accent, and
        so on.
</list>
</P>

<P>
As far as I know, all character codes (besides EBCDIC) in the world 
are compatible with ISO 646.
</P>

<P>
Characters in 0x00 - 0x1f, 0x20, and 0x7f are control characters.
</P>

<P>
Nowadays usage of character codes incompatible with ASCII is not
encouraged and thus ISO 646-* (other than US version) should not 
be used.  One of the reason is that when a string is converted into
Unicode, the converter doesn't know whether IRVs are converted into
characters with same shapes or characters with same codes.  
Another reason is that source codes
are written in ASCII.  Source code must be readable anywhere.
</P>


<sect1 id="iso8859"><heading>ISO 8859</heading>

<P>
<strong>ISO 8859</strong> is both a series of CCS and a series of 
character codes.  It is an expansion of ASCII using all 8 bits.
Additional 96 printable characters encoded in 0xa0 - 0xff are
available besides 94 ASCII printable characters.
</P>

<P>
There are 10 variants of ISO 8859 (in 1997).
<taglist>
 <tag>ISO-8859-1  Latin alphabet No.1 (1987)</tag>
      <item>characters for western European languages
 <tag>ISO-8859-2  Latin alphabet No.2 (1987)</tag>
      <item>characters for central European languages
 <tag>ISO-8859-3  Latin alphabet No.3 (1988)</tag>
 <tag>ISO-8859-4  Latin alphabet No.4 (1988)</tag>
      <item>characters for northern European languages
 <tag>ISO-8859-5  Latin/Cyrillic alphabet (1988)</tag>
 <tag>ISO-8859-6  Latin/Arabic alphabet (1987)</tag>
 <tag>ISO-8859-7  Latin/Greek alphabet (1987)</tag>
 <tag>ISO-8859-8  Latin/Hebrew alphabet (1988)</tag>
 <tag>ISO-8859-9  Latin alphabet No.5 (1989)</tag>
      <item>same as ISO-8859-1 except for Turkish instead of Icelandic
 <tag>ISO-8859-10 Latin alphabet No.6 (1993)</tag>
      <item>Adds Inuit (Greenlandic) and Sami (Lappish) letters to ISO-8859-4
</taglist>
</P>

<P>
A detailed explanation is found at
<url id="http://park.kiev.ua/mutliling/ml-docs/iso-8859.html">.
</P>


<sect1 id="iso-2022"><heading>ISO 2022</heading>

<P>
<strong>ISO 2022</strong> is an international standard of CES.
ISO 2022 determines a few requirement for CCS to be a member
of ISO 2022-based character codes.  It also defines a very
extensive (and complex) rules to combine these CCS into one
character code.  Many character codes such as EUC-*, ISO 2022-*,
compound text,
<footnote>
 Compound text is a standard for text exchange between X clients.
</footnote>
and so on can be regarded as subsets of ISO 2022.
</P>

<P>
The sixth edition of ECMA-35 is fully identical with
ISO 2022:1994 and you can find the official document
at <url id="http://www.ecma.ch/stand/ECMA-035.HTM">.
</P>

<P>
ISO 2022 has two versions of 7bit and 8bit.  At first
8bit version is explained.  7bit version is a subset
of 8bit version.
</P>

<P>
The 8bit code space is divided into four regions,
<list>
 <item>0x00 - 0x1f: C0 (Control Characters 0),
 <item>0x20 - 0x7f: GL (Graphic Characters Left),
 <item>0x80 - 0x9f: C1 (Control Characters 1), and
 <item>0xa0 - 0xff: GR (Graphic Characters Right).
</list>
</P>

<P>
GL and GR is the spaces where (printable) character sets are mapped.
</P>

<P>
Next, all character sets, for example, ASCII, ISO 646-UK,
and JIS X 0208, are classified into following four categories,
<list>
 <item>(1) character set with 1-byte 94-character,
 <item>(2) character set with 1-byte 96-character,
 <item>(3) character set with multibyte 94-character, and
 <item>(4) character set with multibyte 96-character.
</list>
</P>

<P>
Characters in character sets with 94-character are mapped
into 0x21 - 0x7e.  Characters in 96-character set are 
mapped into 0x20 - 0x7f.
</P>

<P>
For example, ASCII, ISO 646-UK, and JIS X 0201 Katakana
are classified into (1), JIS X 0208 Japanese Kanji, 
KS C 5601 Korean, GB 2312-80 Chinese are classified into (3),
and ISO 8859-* are classified to (2).
</P>

<P>
The mechanism to map these character sets into GL and GR is
a bit complex.  There are four buffers, G0, G1, G2, and G3.  
A character set is <strong>designated</strong> into one of these buffers 
and then a buffer is <strong>invoked</strong> into GL or GR.
</P>

<P>
Control sequences to 'designate' a character set into a
buffer are determined as below.
</P>

<P>
<list>
 <item>A sequence to designate a character set with 1-byte 94-character
    <list>
     <item>into G0 set is: ESC 0x28 F,
     <item>into G1 set is: ESC 0x29 F,
     <item>into G2 set is: ESC 0x2a F, and
     <item>into G3 set is: ESC 0x2b F.
    </list>
 <item>A sequence to designate a character set with 1-byte 96-character
    <list>
     <item>into G1 set is: ESC 0x2d F,
     <item>into G2 set is: ESC 0x2e F, and
     <item>into G3 set is: ESC 0x2f F.
    </list>
 <item>A sequence to designate a character set with multibyte 94-character
    <list>
     <item>into G0 set is: ESC 0x24 0x28 F
       (exception: 'ESC 0x24 F' for F = 0x40, 0x41, 0x42.),
     <item>into G1 set is: ESC 0x24 0x29 F,
     <item>into G2 set is: ESC 0x24 0x2a F, and
     <item>into G3 set is: ESC 0x24 0x2b F.
    </list>
 <item>A sequence to designate a character set with multibyte 96-character
    <list>
     <item>into G1 set is: ESC 0x24 0x2d F,
     <item>into G2 set is: ESC 0x24 0x2e F, and
     <item>into G3 set is: ESC 0x24 0x2f F.
    </list>
</list>
where 'F' is determined for each character set:
<list>
 <item>character set with 1-byte 94-character
    <list>
     <item>F=0x40 for ISO 646 IRV: 1983
     <item>F=0x41 for BS 4730 (UK)
     <item>F=0x42 for ANSI X3.4-1968 (ASCII)
     <item>F=0x43 for NATS Primary Set for Finland and Sweden
     <item>F=0x49 for JIS X 0201 Katakana
     <item>F=0x4a for JIS X 0201 Roman (Latin)
     <item>and more
    </list>
 <item>character set with 1-byte 96-character
    <list>
     <item>F=0x41 for ISO 8859-1 Latin-1
     <item>F=0x42 for ISO 8859-2 Latin-2
     <item>F=0x43 for ISO 8859-3 Latin-3
     <item>F=0x44 for ISO 8859-4 Latin-4
     <item>F=0x46 for ISO 8859-7 Latin/Greek
     <item>F=0x47 for ISO 8859-6 Latin/Arabic
     <item>F=0x48 for ISO 8859-8 Latin/Hebrew
     <item>F=0x4c for ISO 8859-5 Latin/Cyrillic
     <item>and more
    </list>
 <item>character set with multibyte 94-character
    <list>
     <item>F=0x40 for JIS X 0208-1978 Japanese
     <item>F=0x41 for GB 2312-80 Chinese
     <item>F=0x42 for JIS X 0208-1983 Japanese
     <item>F=0x43 for KS C 5601 Korean
     <item>F=0x44 for JIS X 0212-1990 Japanese
     <item>F=0x45 for CCITT Extended GB (ISO-IR-165)
     <item>F=0x46 for CNS 11643-1992 Set 1 (Taiwan)
     <item>F=0x48 for CNS 11643-1992 Set 2 (Taiwan)
     <item>F=0x49 for CNS 11643-1992 Set 3 (Taiwan)
     <item>F=0x4a for CNS 11643-1992 Set 4 (Taiwan)
     <item>F=0x4b for CNS 11643-1992 Set 5 (Taiwan)
     <item>F=0x4c for CNS 11643-1992 Set 6 (Taiwan)
     <item>F=0x4d for CNS 11643-1992 Set 7 (Taiwan)
     <item>and more
    </list>
</list>
A more detailed list of these character set is found at
<url id="http://www.kudpc.kyoto-u.ac.jp/~yasuoka/CJK.html">.
<footnote>
WHERE CAN I FIND THE COMPLETE AND AUTHORITATIVE TABLE OF THIS? 
</footnote>
</P>

<P>
Control codes to 'invoke' one of G{0123} into GL or GR
is determined as below.
<list>
 <item>A control code to invoke G0 into GL is: (L)SO ((Locking) Shift Out)
 <item>A control code to invoke G1 into GL is: (L)SO ((Locking) Shift In)
 <item>A control code to invoke G2 into GL is: LS2 (Locking Shift 2)
 <item>A control code to invoke G3 into GL is: LS3 (Locking Shift 3)
 <item>A control code to invoke one character 
                         in G2 into GL is: SS2 (Single Shift 2)
 <item>A control code to invoke one character 
                         in G3 into GL is: SS3 (Single Shift 3)
 <item>A control code to invoke G1 into GR is: LS1R (Locking Shift 1 Right)
 <item>A control code to invoke G2 into GR is: LS2R (Locking Shift 2 Right)
 <item>A control code to invoke G3 into GR is: LS3R (Locking Shift 3 Right)
</list>
<footnote>
WHAT IS THE VALUE OF THESE CONTROL CODES?
</footnote>
</P>

<P>
Note that a character code in a character set invoked into GR is
or-ed with 0x80.
</P>

<P>
ISO 2022 also determines <strong>announcer</strong> code.  For example, 
'ESC 0x20 0x41' means 'Only G0 buffer is used.  G0 is already
invoked into GL'.  This simplify the coding system.  Even this
announcer can be omitted if people who exchange data agree.
</P>

<P>
7bit version of ISO 2022 is a subset of 8bit version.  It does not
use C1 and GR.
</P>

<P>
Explanation on C0 and C1 is omitted here.
</P>


<sect2 id="compound"><heading>Compound Text</heading>

<P>
<strong>Compound Text</strong> is a subset of ISO 2022, 
which is used for X clients to communicate each other,
for example, copy-paste.
</P>

<P>
Compound Text is stateful.
<footnote>
I HAVE TO WRITE EXPLANATION.
</footnote>
</P>



<sect2 id="euc"><heading>EUC (Extended Unix Code)</heading>

<P>
<strong>EUC</strong> is a CES which is a subset of 8bit version 
of ISO 2022 except for the usage of SS2 and SS3 code. Though these
codes are used to invoke G2 and G3 into GL in ISO 2022, they are
invoked into GR in EUC.
<strong>EUC-JP</strong>, <strong>EUC-KR</strong>, <strong>EUC-CN</strong>,
and <strong>EUC-TW</strong> are widely used character codes
which use EUC as CES.
</P>

<P>
EUC is stateless.
</P>

<P>
EUC can contain 4 CCS by using G0, G1, G2, and G3.
Though there is no requirement that ASCII is designated to G0,
I don't know any EUC codeset in which ASCII is not designated to G0.
</P>

<P>
For EUC with G0-ASCII, all codes other than ASCII are encoded 
in 0x80 - 0xff and this is upward compatible to ASCII.
</P>

<P>
Expressions for characters in G0, G1, G2, and G3 character sets
are described below in binary:
<list>
 <item>G0: 0???????
 <item>G1: 1??????? [1??????? [...]]
 <item>G2: SS2 1??????? [1??????? [...]]
 <item>G3: SS3 1??????? [1??????? [...]]
</list>
where SS2 is 0x8e and SS3 is 0x8f.
</P>



<sect1 id="unicodes"><heading>ISO 10646 and Unicode</heading>

<P>
ISO 10646 and Unicode are an another standard so that we can
develop international softwares easily.  The special features
of this new standard are:
<list>
  <item>A united single CCS which intends to include all characters
        in the world.  (ISO 2022 consists of multiple CCS.)
  <item>The character set intends to cover all conventional
        (or <em>legacy</em>) CCS in the world.
        <footnote>
         This is obviously not true for CNS 11643 because
	 CNS 11643 contains 48711 characters while Unicode 3.0.1 
	 contains 49194 characters, only 483 excess than CNS 11643.
	</footnote>
  <item>Compatibility with ASCII and ISO 8859-1 is considered.
  <item>Chinese, Japanese, and Korean ideograms are united.
        This comes from a limitation of Unicode.
        This is not a merit.
</list>
</P>

<P>
ISO 10646 is an official international standard.  Unicode is 
developed by 
<url id="http://www.unicode.org" name="Unicode Consortium">.
These two are almost identical.  Unicode is sometimes updated
and the newest version is 3.0.
</P>

<sect2 id="unicodes-ccs"><heading>UCS as a Coded Character Set</heading>

<P>
ISO 10646 defines two CCS (coded character sets), <strong>UCS-2</strong>
and <strong>UCS-4</strong>.  UCS-2 is a subset of UCS-4.
</P>

<P>
UCS-4 is a 31bit CCS. These 31 bits are divided into 7, 8, 8, and 8 bits
and each of them has special term.
<list>
  <item>The top 7 bits are called <strong>Group</strong>.
  <item>Next 8 bits are called <strong>Plane</strong>.
  <item>Next 8 bits are <strong>Row</strong>.
  <item>The smallest 8 bits are <strong>Cell</strong>.
</list>
The first plane (Group = 0, Plane = 0) is called <strong>BMP</strong> 
(Basic Multilingual Plane) and UCS-2 is same to BMP.
Thus, UCS-2 is a 16bit CCS.
</P>

<P>
Code points in UCS are often expressed as <strong>u+<tt>????</tt></strong>, 
where <tt>????</tt> is hexadecimal expression of the code point.
</P>

<P>
Characters in range of u+0021 - u+007e are same to ASCII and
characters in range of u+0xa0 - u+0xff are same to ISO 8859-1.
Thus it is very easy to convert between ASCII or ISO 8859-1 and UCS.
</P>

<P>
Unicode (version 3.0.1) uses a 20bit subset of UCS-4 as a CCS.
It includes 49194 distinct coded characters.
</P>


<sect2 id="unicode-ces"><heading>UTF as CES</heading>

<P>
A few CES are used to construct character codes which use UCS as
a CCS.  They are <strong>UTF-7</strong>, <strong>UTF-8</strong>, 
<strong>UTF-16</strong>, <strong>UTF-16LE</strong>, and 
<strong>UTF-16BE</strong>.  UTF means Unicode (or UCS)
Transformation Format.
Since these CES always take UCS as the only CCS, they are also
names for character codes.
<footnote>
 Compare UTF and EUC.  There are a few variants of EUC whose CCS
 are different (EUC-JP, EUC-KR, and so on).  This is why we cannot
 call EUC as a character code.  In other words, calling of 'EUC' 
 cannot specify a character code.  On the other hands, 'UTF-8'
 is the name for a specific concrete character code.
</footnote>
</P>

<sect3 id="unicode-utf8"><heading>UTF-8</heading>

<P>
UTF-8 is a character code whose CCS is UCS-4.  UTF-8
is designed to be upward-compatible to ASCII.
UTF-8 is multibyte and number of bytes needed to express
one character is from 1 to 6.
</P>

<P>
Conversion from UCS-4 to UTF-8 is performed using a 
simple conversion rule.
<example>
UCS-4 (binary)                       UTF-8 (binary)
00000000 00000000 00000000 0???????  0???????
00000000 00000000 00000??? ????????  110????? 10??????
00000000 00000000 ???????? ????????  1110???? 10?????? 10??????
00000000 000????? ???????? ????????  11110??? 10?????? 10?????? 10??????
000000?? ???????? ???????? ????????  111110?? 10?????? 10?????? 10?????? 10??????
0??????? ???????? ???????? ????????  1111110? 10?????? 10?????? 10?????? 10?????? 10??????
</example>
</P>

<P>
UTF-8 seems to be one of the major candidates for standard codesets 
in the future.  For example, Linux console and xterm supports UTF-8.
Debian package of <package>locales</package> (version 2.1.97-1)
contains <tt>ko_KR.UTF-8</tt> locale.  I think the number of UTF-8
locale will increase.
</P>

<sect3 id="unicode-utf16"><heading>UTF-16</heading>

<P>
UTF-16 is a character code whose CCS is 20bit Unicode.
</P>

<P>
Characters in BMP are expressed using 16bit value of
code point in Unicode CCS.  There are two ways to express
16bit value in 8bit stream.  Some of you may heard a word
<em>endian</em>.  <em>Big endian</em> means an arrangement
of octets which are part of a datum with many bits
from most significant octet to least significant one.
<em>Little endian</em> is opposite.  For example, 
16bit value of <tt>0x1234</tt> is expressed as 
<tt>0x12 0x34</tt> in
big endian and <tt>0x34 0x12</tt> in little endian.
</P>

<P>
UTF-16 supports both endians.  Thus, Unicode character of
<tt>u+1234</tt> can be expressed either in <tt>0x12 0x34</tt>
or <tt>0x34 0x12</tt>.  Instead, the UTF-16 texts
have to have a <strong>BOM (Byte Order Mark)</strong> at first
of them.  The Unicode character <tt>u+feff</tt> zero width no-break
space is called BOM when it is used to indicate the byte order
or endian of texts.  The mechanism is easy: in big endian,
<tt>u+feff</tt> will be <tt>0xfe 0xff</tt> while it will be
<tt>0xff 0xfe</tt> in little endian.  Thus you can understand
the endian of the text by reading the first two bytes.
</P>

<P>
Characters in out of BMP are expressed using <strong>surrogate
pair</strong>.  Code points of <tt>u+d800</tt> - <tt>u+dfff</tt>
are reserved for this purpose.  At first, 20 bits of Unicode code
point are divided into two sets of 10 bits.  The significant 10 bits
are mapped to 10bit space of <tt>u+d800</tt> - <tt>u+dbff</tt>.
The smaller 10 bits are mapped to 10bit space of <tt>u+dc00</tt> -
<tt>u+dfff</tt>.  Thus UTF-16 can express 20bit Unicode characters.
</P>

<sect3 id="unicode-utf16bele"><heading>UTF-16BE and UTF-16LE</heading>

<P>
UTF-16BE and UTF-16LE are variants of UTF-16 which are limited to
big and little endians, respectively.
</P>


<sect3 id="unicode-utf7"><heading>UTF-7</heading>

<P>
UTF-7 is designed so that Unicode can be communicated using
7bit communication path.
</P>


<sect2 id="unicode-problem"><heading>Problems on Unicode</heading>

<P>
All standards are not free from politics and compromise.
Though a concept of united single CCS for all characters in the
world is very nice, Unicode had to consider compatibility
with preceding international and local standards.  And more,
unlike the ideal concept, Unicode people considered efficiency
too much.  IMHO, surrogate pair is a mess caused by lack of
16bit code space.  I will introduce a few problems on Unicode.
</P>

<sect3 id="unihan"><heading>Han Unification</heading>

<P>
This is the point on which Unicode is criticized most strongly
among many Japanese (and also among Korean and Chinese, I suppose)
people.
</P>

<P>
A region of 0x4e00 - 0x9fff in UCS-2 is used for Eastern-Asian
ideographs (Japanese Kanji, Chinese Hanzi, and Korean Hanja).  
There are similar characters
in these four character sets. (There are two sets of Chinese characters, 
simplified Chinese used in P. R. China and traditional Chinese used in 
Taiwan).  To reduce the number of these ideograms to be encoded
(the region for these characters can contain only 20992 characters
while only Taiwan CNS 11643 standard contains 48711 characters),
these similar characters are assumed to be the same.
This is Han Unification.
</P>

<P>
However these characters are not exactly the same.  If fonts for
these characters are made from Chinese one, Japanese people will
regard them wrong characters, though they may be able to read.
Unicode people think these united characters are the same character
with different glyphs.
</P>

<P>
An example of Han Unification is available at
<url id="http://charts.unicode.org/unihan/unihan.acgi$0x9AA8">.
This is a Kanji character for 'bone'. 
<url id="http://charts.unicode.org/unihan/unihan.acgi$0x8FCE">
is an another example of a Kanji character for 'welcome'.
</P>

<P>
Unicode font vendors will hesitate to choose fonts for these characters,
simplified Chinese character, traditional Chinese one, Japanese one, or 
Korean one.  One method is to supply four fonts of simplified Chinese 
version, traditional Chinese version, Japanese version, and Korean version.
Commercial OS vender can release localized version of their OS ---
for example, Japanese version of MS Windows can include Japanese version
of Unicode font (this is what they are exactly doing).  However, how 
should XFree86 or Debian do?  I don't know...
<footnote>
  XFree86 4.0 includes Japanese and Korean versions of ISO 10646-1 fonts.
</footnote>
</P>

<sect3 id="combining"><heading>Combining Characters</heading>

<P>
Unicode has a way to synthesize a accented character by combining
an accent symbol and a base character.  For example, combining 'a' and
'~' makes 'a' with tilde.  More than two accent symbol can be added to
a base character.  
</P>

<P>
Languages such as Thai need combining characters.  Combining characters
are the only method to express characters in these languages.
</P>

<P>
However, a few problems arises.
<taglist>
 <tag>Duplicate Encoding</tag>
    <item>
    There are multiple ways to express the same character.
    For example, u with umlaut can be expressed as <tt>u+00fc</tt>
    and also as <tt>u+0075</tt> + <tt>U+0308</tt>.
    How can we implement 'grep' and so on?
 <tag>Open Repertoire</tag>
    <item>
    Number of expressible characters grows unlimitedly.
    Non-existing characters can be expressed.
</taglist>
</P>


<sect3 id="surrogate"><heading>Surrogate Pair</heading>

<P>
The first version of Unicode had only 16bit code space,
though 16bit is obviously insufficient to contain all
characters in the world.
<footnote>
  There are a few projects such as
  <url id="http://www.mojikyo.gr.jp/" name="Mojikyo">
  (about 90000 characters),
  <url id="http://www.tron.org/index-e.html" name="TRON project">
  (about 130000 characters),
  and so on to develop a CCS which contains
  sufficient characters for professional usage in CJK world.
</footnote>
Thus surrogate pair is introduced in Unicode 2.0, to expand the
number of characters, with keeping compatibility with former
16bit Unicode.
</P>

<P>
However, surrogate pair breaks the principle that all characters
are expressed with the same width of bits.  This makes Unicode
programming more difficult.
</P>


<sect3 id="646problem"><heading>ISO 646-* Problem</heading>

<P>
You will need a codeset converter between your local character codes
(for example, ISO 8859-* or ISO 2022-*) and Unicode.
For example, Shift-JIS character code
<footnote>
  The standard character code for Macintosh and MS Windows.
</footnote>
consists from
JISX 0201 Roman (Japanese version of ISO 646), not ASCII,
which encodes yen currency mark at <tt>0x5c</tt>
where backslash is encoded in ASCII.  
</P>

<P>
Then which should your converter convert <tt>0x5c</tt> in Shift-JIS
into in Unicode, <tt>u+005c</tt> (backslash) or <tt>u+00a5</tt> 
(yen currency mark)?
You may say yen currency mark is the right solution.
However, backslash (and then yen mark) is widely used for
escape character. For example, 'new line' is expressed as
'backslash - <tt>n</tt>' in C string literal and Japanese people use
'yen currency mark - <tt>n</tt>'.  You may say that program sources
must written in ASCII and the wrong point is that you 
tried to convert program source.  However, there are many
source codes and so on written in Shift-JIS character code.
</P>

<P>
Now Windows comes to support Unicode and the font
at <tt>u+005c</tt> for Japanese version of Windows is yen currency mark.
As you know, backslash (yen currency mark in Japan) is vitally 
important for Windows, because it is used to separate directory names.
Fortunately, EUC-JP, which is widely used for UNIX in Japan,
includes ASCII, not Japanese version of ISO 646.  So this 
is not problem because it is clear <tt>0x5c</tt> is backslash.
</P>

<P>
Thus all local codesets should not use character sets incompatible
to ASCII, such as ISO 646-*.
</P>


<sect1 id="othercodes"><heading>Other Character Codes</heading>

<P>
There are a few popular character codes which cannot be classified
into an international standard.  Internationalized softwares should
support these character codes (again, you don't need to be aware of
character codes if you use LOCALE and <tt>wchar_t</tt> technology).
Some organizations are developing systems which go father than 
limitations of the current international standards, though these
systems may be not diffused very much so far.
</P>

<sect2 id="othercodes-big5"><heading>Big5</heading>

<P>
<strong>Big5</strong> is a de-facto standard character code for 
Taiwan (1984).  It is also a CCS which is upper-compatible with ASCII.
</P>

<P>
In Big5, <tt>0x21</tt> - <tt>0x7e</tt> means ASCII characters.
<tt>0xa1</tt> - <tt>0xfe</tt> makes a pair with the following byte
(<tt>0x40</tt> - <tt>0x7e</tt> and <tt>0xa1</tt> - <tt>0xfe</tt>)
and means an ideogram and so on (13461 characters).  
<P>

<P>
Though Taiwan has ISO 2022-compliant new standard CNS 11643,
Big5 seems to be more popular than CNS 11643.
(CNS 11643 is a CCS and there are a few ISO 2022-derived
character codes which include CNS 11643.)
</P>

<sect2 id="othercodes-viscii"><heading>VISCII</heading>

<P>
Vietnamese language uses 186 characters (Latin alphabets with accents).
It is a bit more than the limit of ISO 8859-like character code.
</P>

<P>
<strong>VISCII</strong> is a standard for Vietnamese.  
It is upper-compatible with ASCII.  It is 8bit and stateless, 
like ISO 8859 series.  However, it uses code points of
not only <tt>0x21</tt> - <tt>0x7e</tt> and <tt>0xa0</tt> - 
<tt>0xff</tt> but also <tt>0x02</tt>, <tt>0x05</tt>, <tt>0x06</tt>, 
<tt>0x14</tt>, <tt>0x19</tt>, <tt>0x1e</tt>, and <tt>0x80</tt> -
<tt>0x9f</tt>.  This makes VISCII not-ISO 2022-compliant.
</P>

<P>
Vietnam has a new, ISO 2022-compliant character code
<strong>TCVN 5712</strong> (aka <strong>VSCII</strong>).
In TCVN 5712, accented characters are expressed as a
combined character.  Note that a part of accented characters
have their own code points.
</P>

<sect2 id="othercodes-tron"><heading>TRON</heading>

<P>
url id="http://www.tron.org/index-e.html" name="TRON project">
is a project to develop a new operating system,
founded as a collaboration of industries and academics
in Japan since 1984.
</P>

<P>
The most diffused version of TRON operating system families
is ITRON, a real-time OS for embedded systems.
However, our interest is not on the ITRON now.
TRON determines a TRON character code.
</P>

<P>
TRON's character code is stateful.  Each state are assigned
to each language.  It has already defined about 130000 characters
(January 2000).
</P>

<sect2 id="othercodes-mojikyo"><heading>Mojikyo</heading>

<P>
<url id="http://www.mojikyo.gr.jp/" name="Mojikyo">
is an project to develop an environment by which a user
can use many characters in the world.  Mojikyo
project has released an application software for
MS Windows to display and input about 90000 characters.
You can download the software and TrueType, TeX, and
CID fonts, though they are not DFSG-free.
</P>



<chapt id="languages"><heading>Characters in Each Country</heading>

<P>
This chapter describes a specific information for each language.
Contributions from people speaking each language are welcome.
If you are to write a section on your language, please include
these points:
<enumlist>
  <item>kinds and number of characters used in the language,
  <item>explanation on coded character set(s) which is (are) standardized,
  <item>explanation on character code(s) which is (are) standardized,
  <item>usage and popularity for each character code,
  <item>de-facto standard, if any, on how many columns characters occupy, 
  <item>writing direction and combined characters,
  <item>how to layout characters (word wrapping and so on),
  <item>widely used value for <tt>LANG</tt> environmental variable,
  <item>the way to input characters from keyboard and whether
        you want to input yes/no (and so on) in your language 
        or in English,
  <item>a set of information needed for beautiful displaying, for example, 
        where to break a line, hyphenation, word wrapping, and so on, and
  <item>other topics.
</enumlist>
</P>


<P>
Writers whose languages are written in different direction
from European languages or needs a combined characters
(I heard that is used in Thai) are encouraged to explain 
how to treat such languages.
</P>



&japanese-japan;
&spanish;






<chapt id="locale"><heading>LOCALE technology</heading>

<P>
<strong>LOCALE</strong> is a basic concept introduced
into <strong>ISO C</strong> (ISO/IEC 9899:1990).  The 
standard is expanded in 1995 (ISO 9899:1990 Ammendment 1:1995).  
In LOCALE model, the behaviors of some C functions are dependent
on LOCALE environment.  LOCALE environment is divided
into a few categories and each of these categories can
be set independently using <tt>setlocale()</tt>.
</P>

<P>
<strong>POSIX</strong> also determines some standards around 
i18n.  Almost of POSIX and ISO C standards are included in
<strong>XPG4</strong> (X/Open Portability Guide) standard and
all of them are included in XPG5 standard.  Note that XPG5 is
included in UNIX specifications version 2.  Thus support of
XPG5 is mandatory to obtain Unix brand.  In other words,
all versions of Unix operating systems support XPG5.
</P>

<sect id="localecategory">Locale Categories and Locale Names</heading>

<P>
In LOCALE model, the behaviors of some C functions are dependent
on LOCALE environment.  LOCALE environment is divided
into six categories and each of these categories can
be set independently using <tt>setlocale()</tt>.
</P>

<P>
The followings are the six categories:
<taglist>
  <tag><strong>LC_CTYPE</strong>
       <item>
       <p>
       Category related to character code.
       Characters which are encoded by LC_CTYPE-depndent character
       code is called <strong>multibyte characters</strong>.
       Note that multibyte character doesn't need to be multibyte.
       </p>
       <p>
       LC_CTYPE-dependent functions are: character testing functions
       such as <tt>islower()</tt> and so on, multibyte character
       functions such as <tt>mblen()</tt> and so on, multibyte
       string functions such as <tt>mbstowcs()</tt> and so on,
       and so on.
       </p>
       </item>
  <tag><strong>LC_COLLATE</strong>
       <item>
       <p>
       Category related to sorting.
       <tt>strcoll()</tt> and so on are LC_COLLATE-dependent.
       </p>
       </item>
  <tag><strong>LC_MESSAGES</strong>
       <item>
       <p>
       Category related to the language for messages the software
       outputs.  This category is used for <prgn>gettext</prgn>.
       </p>
  <tag><strong>LC_MONETARY</strong>
       <item>
       <p>
       Category related to format to show monetary numbers,
       for example, currency mark, comma or period, columns,
       and so on.
       <tt>localeconv()</tt> is the only function which is
       LC_MONETARY-dependent.
       </p>
       </item>
  <tag><strong>LC_NUMERIC</strong>
       <item>
       <p>
       Category related to format to show general numbers,
       for example, character for decimal point.
       </p>
       <p>
       Formatted I/O functions such as <tt>printf()</tt>,
       string conversion functions such as <tt>atof()</tt>,
       and so on are LC_NUMERIC-dependent.
       </p>
       </item>
  <tag><strong>LC_TIME</strong>
       <item>
       <p>
       Category related to format to show time and date,
       such as name of months and weeks, order of date,
       month, and year, and so on.
       </p>
       <p>
       <tt>strftime()</tt> and so on are LC_TIME-dependent.
       </p>
       </item>
</taglist>
</p>

<p>
<tt>setlocale()</tt> is a function to set LOCALE.
Usage is char *<tt>setlocale(</tt>int <em>category</em>, const char 
*<em>locale</em><tt>);</tt>.  Header file of <tt>locale.h</tt>
is needed for prototype declaration and definition of
macros for category names.  For example,
<tt>setlocale(LC_TIME, "de_DE");</tt>.
</p>

<p>
For <em>category</em>, the following macros can be used:
LC_CTYPE, LC_COLLATE, LC_MONETARY, LC_NUMERIC, LC_TIME, and
LC_ALL.  For <em>locale</em>, specific locale name, <tt>NULL</tt>,
or <tt>""</tt> can be specified.
</p>

<p>
Giving <tt>NULL</tt> for <em>locale</em> will return the
current value of the specified locale category.  Otherwise,
<tt>setlocale()</tt> returns the newly set locale name,
or <tt>NULL</tt> for error.
</p>

<p>
Given <tt>""</tt> for <em>locale</em>, <tt>setlocale()</tt>
will determine the locale name in the following manner:
<list>
  <item>At first, consult <tt>LC_ALL</tt> environmental variable.
  <item>Then, consult environmental variable same as the 
        name of the locale category.  For example, <tt>LC_COLLATE</tt>.
  <item>At last, consult <tt>LANG</tt> environmental variable.
</list>
This is why a user is expected to set <tt>LANG</tt> variable.
In other words, all what a user has to do is to set <tt>LANG</tt>
variable so that all locale-compliant softwares work well for
desired way.
</p>

<p>
Thus, I recommend strongly to call <tt>setlocale(LC_ALL, "");</tt>
at the first of your softwares, if the softwares are to be 
international.
</p>

<p>
Note that locale names of <tt>"C"</tt> and <tt>"POSIX"</tt> are
determined for the names for default behavior.  For example,
when your software need to parse the output of <tt>date(1)</tt>,
you'd better call <tt>setlocale(LC_TIME, "C");</tt> before 
invocation of <tt>date(1)</tt>.
</p>

<sect id="wchar">Multibyte Characters and Wide Characters</heading>

<p>
Now we will concentrate on LC_CTYPE category.
</p>

<p>
Many character codes such as ASCII, ISO 8859-*, KOI8-R, EUC-*,
ISO 2022-*, TIS 620, UTF-8, and so on are used widely in the world.
It is inefficient and a cause of bugs, even not impossible, for
every softwares to implement all these character codes.
Fortunetely, we can use LOCALE technology to solve this problem.
<footnote>
  Usage of UCS-4 is the second best solution fot this problem.
  Sometimes LOCALE technology cannot be used and UCS-4 is the
  best.  I will discuss this solution later.
</footnote>
</p>

<p>
<strong>Multibyte characters</strong> is a term to call characters
encoded in locale-specific character code.  Thus, the behaviors of
C functions which handle multibyte characters depend on
<tt>LC_CTYPE</tt> locale category.
Multibyte characters should be used when your software inputs
or outputs text data from/to everywhere out of your software,
for example, standard input/output, display, keyboard, file,
and so on.
<footnote>
 There are a few exceptions.  Compound text should be used for
 communication between X clients.  UTF-8 would be the standard
 for file names in Linux.
</footnote>
</p>

<p>
Multibyte character may be stateful or stateless and multibyte or
non-multibyte.  Thus it is not convenient for internal processing.
It needs complex algorithm even for, for example, character
extraction from a string, addition and division of a string, 
or counting of number of character in a string.
Thus, <strong>wide characters</strong> should be used for internal
processing.
</p>

<p>
There are two types for wide characters: <tt>wchar_t</tt> and
<tt>wint_t</tt>.  <tt>wchar_t</tt> is a type which can contain
one wide character.  It is just like 'char' type can be used for
contain one character.  <tt>wint_t</tt> can contain one wide
character and <tt>WEOF</tt>, an substitution of <tt>EOF</tt>.
</p>

<p>
A string of wide characters is achived by an array of <tt>wchar_t</tt>,
just like a string of characters is achieved by an array
of <tt>char</tt>.
</p>

<p>
There are functions for <tt>wchar_t</tt>, substitute for functions
for <tt>char</tt>.
<list>
  <item><tt>strcat()</tt>, <tt>strncat()</tt> -&gt;
        <tt>wcscat()</tt>, <tt>wcsncat()</tt>
  <item><tt>strcpy()</tt>, <tt>strncpy()</tt> -&gt;
        <tt>wcscpy()</tt>, <tt>wcsncpy()</tt>
  <item><tt>strcmp()</tt>, <tt>strncmp()</tt> -&gt;
        <tt>wcscmp()</tt>, <tt>wcsncmp()</tt>
  <item><tt>strcasecmp()</tt>, <tt>strncasecmp()</tt> -&gt;
        <tt>wcscasecmp()</tt>, <tt>wcsncasecmp()</tt>
  <item><tt>strcoll()</tt>, <tt>strxfrm()</tt> -&gt;
        <tt>wcscoll()</tt>, <tt>wcsxfrm()</tt>
  <item><tt>strchr()</tt>, <tt>strrchr()</tt> -&gt;
        <tt>wcschr()</tt>, <tt>wcsrchr()</tt>
  <item><tt>strstr()</tt>, <tt>strpbrk()</tt> -&gt;
        <tt>wcsstr()</tt>, <tt>wcspbrk()</tt>
  <item><tt>strtok()</tt>, <tt>strspn()</tt>, <tt>strcspn()</tt> -&gt;
        <tt>wcstok()</tt>, <tt>wcsspn()</tt>, <tt>wcscspn()</tt>
  <item><tt>strtol()</tt>, <tt>strtoul()</tt>, <tt>strtod()</tt> -&gt;
        <tt>wcstol()</tt>, <tt>wcstoul()</tt>, <tt>wcstod()</tt>
  <item><tt>strftime()</tt> -&gt;
        <tt>wcsftime()</tt>
  <item><tt>strlen()</tt> -&gt;
        <tt>wcslen()</tt>
  <item><tt>toupper()</tt>, <tt>tolower()</tt> -&gt;
        <tt>towupper()</tt>, <tt>towlower()</tt>
  <item><tt>isalnum()</tt>, <tt>isalpha()</tt>, <tt>isblank()</tt>,
	<tt>iscntrl()</tt>, <tt>isdigit()</tt>, <tt>isgraph()</tt>,
	<tt>islower()</tt>, <tt>isprint()</tt>, <tt>ispunct()</tt>,
	<tt>isspace()</tt>, <tt>isupper()</tt>, <tt>isxdigit()</tt> -&gt;
	<tt>iswalnum()</tt>, <tt>iswalpha()</tt>, <tt>iswblank()</tt>,
	<tt>iswcntrl()</tt>, <tt>iswdigit()</tt>, <tt>iswgraph()</tt>,
	<tt>iswlower()</tt>, <tt>iswprint()</tt>, <tt>iswpunct()</tt>,
	<tt>iswspace()</tt>, <tt>iswupper()</tt>, <tt>iswxdigit()</tt>
	(<tt>isascii()</tt> doesn't have its wide character version).
  <item><tt>memset()</tt>, <tt>memcpy()</tt>, <tt>memmove</tt>,
	<tt>memmove()</tt>, <tt>memchr()</tt> -&gt;
	<tt>wmemset()</tt>, <tt>wmemcpy()</tt>, <tt>wmemmove</tt>,
	<tt>wmemmove()</tt>, <tt>wmemchr()</tt> 
</list>
There are additional functions for <tt>wchar_t</tt>.
<list>
  <item><tt>wcwidth()</tt>, <tt>wcswidth()</tt>
  <item><tt>wctrans()</tt>, <tt>towctrans()</tt>
</list>
</p>

<p>
You cannot assume anything on the concrete value of <tt>wchar_t</tt>,
besides <tt>0x21</tt> - <tt>0x7e</tt> are identical to ASCII.
</p>

<p>
You can write wide character in the source code as <tt>L'a'</tt>
and wide string as <tt>L"string"</tt>.  Since the character
code for the source code is ASCII, you can only write ASCII
characters.  If you'd like to use other characters, you should
use <prgn>gettext</prgn>.
</p>

<p>
There are two ways to use wide characters:
<list>
  <item>I/O is described using multibyte characters.  Inputed data
        are converted into wide character immediately after reading
        and data for output are converted from wide character to 
	multibyte character immediately before writing.  Conversion
	can be achieved using functions of <tt>mbstowcs()</tt>,
	<tt>mbsrtowcs()</tt>, <tt>wcstombs()</tt>, <tt>wcsrtombs()</tt>,
	<tt>mblen()</tt>, <tt>mbrlen()</tt>, <tt>mbsinit()</tt>,
	and so on.  
	Please consult the manual pages for these functions.
  <item>Wide characters are directly used for I/O, using 
	wide character functions such as <tt>getwchar()</tt>, 
	<tt>fgetwc()</tt>, <tt>getwc()</tt>,
	<tt>ungetwc()</tt>, <tt>fgetws</tt>, <tt>putwchar()</tt>,
	<tt>fputwc()</tt>, <tt>putwc()</tt>, and <tt>fputws()</tt>,
	formatted I/O functions for wide characters such as
	<tt>fwscanf()</tt>, <tt>wscanf()</tt>, <tt>swscanf()</tt>,
	<tt>fwprintf()</tt>, <tt>wprintf()</tt>, <tt>swprintf()</tt>,
	<tt>vfwprintf()</tt>, <tt>vwprintf()</tt>, and
	<tt>vswprintf()</tt>, and wide character identifier
	of <tt>%lc</tt>, <tt>%C</tt>, <tt>%ls</tt>, <tt>%S</tt>
	for conventional formatted I/O functions.
	Please consult the manual pages for these functions.
</list>
Though latter functions are also determined in ISO C,
these functions have became newly available since GNU libc 2.2.
(Of course all UNIX operating systems have all functions described
here.)
</p>


<p>
<strong>****** I HAVE REWRITTEN THIS DOCUMENT UNTIL HERE ******</strong>
</p>



<sect id="iconv">nl_langinfo() and iconv()</heading>

<p>
</p>

<sect id="locale_unicode">Unicode and LOCALE technology</heading>

<p>
UTF-8 is considered as the future character code and
many softwares are coming to support UTF-8.  Though some
of these softwares implement UTF-8 directly, I recommend
you to use LOCALE technology to support UTF-8.
</p>

<p>
How this can be achieved?  It is easy!  If you are a developer
of a software and your software has already written using LOCALE
technology, you don't have to do anything!
</p>

<p>
Using LOCALE technology benefits not only developers but also users.
All a user has to do is set locale environment properly.
Otherwise, a user has to remember the method to use UTF-8 mode
for each software.  Some softwares need <tt>-u8</tt> switch,
other need X resource setting, other need <tt>.foobarrc</tt>
file, other need a special environmental variable,
other use UTF-8 for default.  It is nonsense!
</p>

<p>
Solaris has been already developed using this model.
Please consult 
<url id="http://docs.sun.com/ab2/coll.651.1/SOLUNICOSUPPT" 
name="Unicode support in the Solaris Operating Environment"> whitepapaer.
</p>

<p>
However, it is likely that some of upstream developers of
softwares of which you are maintaining a Debian package refuses
to use <tt>wchar_t</tt> for some reasons, for example, that
they are not familiar with LOCALE programming, that they think
it is troublesome, that they are not keen on I18N, that it is much
easier to modify the software to support UTF-8 than to modify it
to use <tt>wchar_t</tt>, that the software must work even under
non-internationalized OS such as MS-DOS, and so on.
Some developers may think that support of UTF-8 is sufficient
for I18N.
<footnote>
 In such a case, do they think of abolishing support of 7bit or
 8bit non-multibyte character codes?  If no, it may be unfair that
 8bit language speakers can use both UTF-8 and conventional (local)
 character codes while speakers of multibyte languages, combining
 characters, and so on cannot use their popular locale character
 codes.  I think such a software cannot be called "internationalized".
</footnote>
Even in such cases, you can rewrite such a software so that it 
checks <tt>LC_*</tt> and <tt>LANG</tt> environmental variables
to emulate the behavior of <tt>setlocale(LC_ALL, "");</tt>.  
You can also rewrite the software to call <tt>setlocale()</tt>,
<tt>nl_langinfo()</tt>, and <tt>iconv()</tt> so that the software
supports all character codes which the OS supports.
Consult
<url id="http://ffii.org/archive/mails/groff/2000/Oct/0056.html"
name="the discussion in the Groff mailing list on the support of
UTF-8 and locale-specific character codes">, mainly held by Werner
LEMBERG, an experienced developer of GNU roff, and Tomohiro KUBOTA,
the author of this document.
</p>









<chapt id="output"><heading>Output to Display</heading>

<P>
Here 'Output to Display' does not mean I18N of messages using 
<prgn>gettext</prgn>.
I will concern on whether characters are correctly outputed so that
we can read it.  For example, install <package>libcanna1g</package> 
package and display
<tt>/usr/doc/libcanna1g/README.jp.gz</tt> on console or <prgn>xterm</prgn>
 (of course after
ungzipping).  This text file is written in Japanese but even Japanese
people can not read such a row of strange characters.  Which you would
prefer if you were a Japanese speaker, an English message which can be read
with a dictionary or such a row of strange characters which is 
a result of <prgn>gettext</prgn>ization?  
(Yes, there <em>is</em> a way to display 
Japanese characters correctly -- <prgn>kon</prgn> (in <package>kon2</package>
package)  for console and <prgn>kterm</prgn> for X, and 
Japanese people are happy with <prgn>gettext</prgn>ized Japanese messages.)
</P>

<P>
Problems on displaying non-English characters are discussed below.
Since the mother tongue of the author is Japanese, the content may
be biased to Japanese.
</P>



<sect id="output-console"><heading>Console Softwares</heading>

<P>
Softwares running on the console are not responsible for displaying.
The console itself is responsible.  There are terminal emulators
which can display non-English languages such as <prgn>kterm</prgn> 
(EUC-JP, Shift JIS Japanese, ISO 2022 international),
<prgn>krxvt</prgn>, <prgn>grxvt</prgn>, and <prgn>crxvt</prgn> 
(Japanese, Greek, and Chinese, included
in <package>rxvt-ml</package> package), <prgn>cxterm</prgn> 
(Chinese, Korean, and Japanese, non-free), <prgn>hanterm</prgn>
(Korean)
and so on and softwares with which non-English characters can be 
displayed on console such as <package>kon2</package> (Japanese)
and <package>jfbterm</package> (EUC-based character codes and
ISO2022-based character codes).
</P>

<P>
All what a software on console/terminal-emulators
has to do is that output a correct code to the console. 
</P>

<P>
At first, it is important not to destroy string data.
Sometimes it can be done only by 8bit-clean-ize.
'8bit-clean' means that the software does not destroy the
most significant bit (MSB) of data the software treats.
</P>

<P>
Next, be careful for a software which sends control codes such
as location every time it output 1 byte.  Such codes destroy
the continuity of multibyte character.  
</P>

<P>
Be also careful for destruction of multicolumn characters.
For example, when a string exceeds the width of the console, 
the string is divided at the end of the line.  Terminal emulators
should have a faculty to avoid such a 'excess of line width' type
destruction of character but so far no terminal emulators
have such a faculty.  (Only one exception --- shell mode of Emacs.
However, unfortunately shell mode of Emacs is a dumb terminal and 
many softwares cannot be run on it.)  Thus each software on 
console should be careful.
</P>

<P>
There is another reason to destroy multicolumn characters.
When a message is overwritten on another string, a part
of a character which is a part of a previous string can be
left not overwritten.  This may be more troublesome than many
people would think because multicolumn character can be
written at every columns, not only at the multiple of the
width of the character.
</P>

<P>
These destruction of continuity of multibyte characters may 
be a cause of the destruction of the whole line following 
the character.  Whether this can occur depends on the internal
implementation of console program.  This can occur if the
terminal emulator does not treat columns, bytes and characters
properly separately.  The shell mode of Emacs is the only example
doing that but there are no chance to overwrite character on
the shell mode of Emacs, because it is a dumb terminal.
</P>

<P>
There are no standards for number of columns a character occupies.
This can be a large problem for softwares with <tt>ncurses</tt>.
There is no 'right' way to solve this.  Each software has to
have an information for each character set.  Consult section 
<ref id="languages">
for each language.  Take care of the distinction between number 
of columns, bytes, and characters.  For subset of EUC-JP
(ASCII alphabets and JIS X 0208 kanji), number of bytes and columns 
are equal (1-byte character occupy 1 column and 2-byte character
occupy 2 columns).  Note that cursor-moving control characters
such as 'BS' (0x08) moves cursor one COLUMN, not one CHARACTER.
</P>

<P>
Another important point is that the string has to be converted 
into a character code which the console can understand.  So far there
are no consoles which understand Unicode.
</P>



<sect id="output-x"><heading>X Clients</heading>

<P>
X itself is already internationalized.  X11R5 has introduced 
an idea of 'fontset' for internationalized text output.
Thus all what X clients have to do is to use the 'fontset'-related
functions.
</P>

<P>
An X font is related to a specific <em>character set</em>.  The
conventional font-related functions can use one font at the same time.
However, text is expressed in a specific <em>character code</em> and
some character codes need multiple character sets.  Chinese, Japanese,
and Korean are languages which need multiple character sets.
These languages cannot be displayed using the conventional
font-related functions.
</P>

<P>
'fontset' is an idea that multiple fonts are selected and construct
a set of fonts.  Using fontset enables to display international texts.
</P>

<P>
Here is a list of structure and functions of conventional 'Font'-related
and internationalized 'FontSet'-related.  Consult manpages for detail.
<example>
Font              | FontSet
==================+====================
XFontStruct       | XFontSet
------------------+--------------------
XLoadFont()       | XCreateFontSet()
------------------+--------------------
XUnloadFont()     | XFreeFontSet()
------------------+--------------------
XQueryFont()      | XFontsOfFontSet()
------------------+--------------------
XDrawString() and | XmbDrawString() or
XDrawString16()   | XwcDrawString()
------------------+--------------------
XDrawText() and   | XmbDrawText() or
XDrawText16()     | XwcDrawText()
------------------+--------------------
</example>
</P>

<P>
If a software uses the left-hand functions it have to be rewritten
using the corresponding right-hand functions in the table.  Note that
this table is not perfect but only for an example.  Since these 
right-hand functions use wide characters and multibyte characters
in C, <tt>setlocale()</tt> has to be called in advance.
</P>

<P>
Some people (ISO-8859-1-language speakers) may think that 
XFontSet is not 8-bit clean.  This is wrong.  XFontSet-related
functions work according to LC_CTYPE locale.  The default LC_CTYPE
locale uses ASCII.  Thus, if a user doesn't set <tt>LANG</tt>,
<tt>LC_CTYPE</tt>, nor <tt>LC_ALL</tt> environmental variable, 
XFontSet will use ASCII, i.e., not 8-bit clean.  The user 
has to set <tt>LANG</tt>, <tt>LC_CTYPE</tt>, or <tt>LC_ALL</tt> 
environmental variable properly (for example, <tt>LANG=en_US</tt>).
</P>

<P>
The upstream developers of X clients sometimes hate to enforce
users to set such environmental variables.  In such a case,
The X clients should have two ways to output text, i.e., 
XFontStruct-related conventional way and XFontSet-related
internationalized way.  If <tt>setlocale()</tt> returns
<tt>NULL</tt>, <tt>"C"</tt>, or <tt>"POSIX"</tt>, use 
XFontStruct way.  Otherwise use XFontSet way.  This algorithm
is adopted by <package>Blackbox</package> (0.60.1 or later).
</P>

<P>
The same problem exists for softwares using toolkits such as
athena, GTK+, Qt, and so on.
</P>







<chapt id="input"><heading>Input from Keyboard</heading>

<P>
I18N of display is a prerequisite for I18N of input from keyboard.
I18N is sometimes not necessary for answering Yes/No.  For example, most
Japanese-speaking people regard it is too troublesome only for 
answer Y/N to invoke the input method, input alphabetical 
representation of Japanese, and convert to Japanese character. 
This would be true for Korean and Chinese.  On the other hand
softwares such as text editor, word processor, terminal emulator,
and shell should have I18N-ed input support, though I recommend
M17N than I18N for word processors.  (I can write Japanese-Korean
dictionary with a M17N-ed word processor but I cannot do with
an I18N-ed one.)
</P>



<sect id="input-console"><heading>Console Softwares</heading>

<sect1 id="input-console-console"><heading>Invoked in the Console and Kon</heading>

<P>
These are no standards for international input in console environment.
For example, Canna and Wnn are client/server type Japanese input methods.
Wnn has its variants for Korean and Chinese.
They have their own protocols and there are no standards.
There are softwares to add a faculty of inputting Japanese
to console by connecting console and these input methods,
but these softwares (canuum for Canna and uum for Wnn) are 
not Debianized yet.  There are a few softwares which can talk 
Canna or Wnn protocol directly, for example, nvi-m17n-canna.
In Debian system, these softwares 'depends' on libcanna or wnn 
packages.
</P>

<P>
GNU Emacs offers methods for inputting many languages
such as Japanese, Chinese, Korean, Latin-{12345}, Russian,
Greek, Hebrew, Thai, Vietnamese, Czech, and so on
in the console environment.  XEmacs also offers similar
mechanism but the set of supported languages are different.
We will be very happy if the input faculty of (X)Emacs 
becomes a library and other softwares can use.  The author
doesn't know this can be achieved or not.
</P>

<P>
After an input method is supplied, 
inputed codes must be treated correctly.
That is, the software must be aware of the number
of bytes, characters, and columns.
For example, you have to know how many bytes should be
deleted and how many '^H' code should be sent to console
when 'BS' key is pushed.
</P>


<sect1 id="input-console-x"><heading>Invoked in an X Terminal Emulator</heading>

<P>
X has a standard to input various languages.  That is XIM,
which is standardized at X11R6.  X11R5 standards (Xsi and Ximp)
are not recommended.
Kinput2 is a software to connect Canna and/or Wnn and XIM protocol.
And more, terminal emulators such as kterm and krxvt have a
faculty to connect to XIM.  So the way to input various languages
is supplied.
</P>

<P>
All what softwares running on a terminal emulator have to do is
to accept the input properly.
</P>

<P>
At first 8bit-clean-ize is needed.  
Important softwares such as <prgn>bash</prgn> and <prgn>tcsh</prgn> 
are already 8bit-clean-ized
and they accept non-ASCII characters.
</P>

<P>
However, many softwares (including libreadline) aren't conscious of
multibyte and combined characters.  It is a bit hard to edit inputed
strings including multibyte/combined characters on these softwares.
Assume you pushed 'BackSpace' key.  If the character before the cursor
is a multibyte character, the software should delete two or more bytes
from the internal buffer.  If the character before the cursor occupies
two or more columns, the software has to send two or more backspace
codes to the terminal.  Thus you can hardly edit strings written in
Japanese, Korean, Chinese, Thai, Unicode, and so on.
</P>




<sect id="input-x"><heading>X Clients</heading>

<P>
All you need is that:
<list>
 <item>To accept input from XIM.  'Over-the-spot' conversion is desirable but
       not essential.
 <item>To accept 'paste' using Compound Text.
</list>
</P>







<chapt id="internal"><heading>Internal Processing and File I/O</heading>

<P>
From a user's point of view, a software can use any internal character
codes if I/O is done correctly.  It is because a user cannot be aware of
which kind of internal code is used in the software.
</P>

<P>
It is recommended to use wide character (<tt>wchar_t</tt>) as an
internal code.  The strings obtained from file, keyboard, and so on
are to be converted into wide characters using <tt>mbstowcs</tt> and
so on and then to be processed.  When the string is to be outputed,
the string is to be converted into multibyte characters using
<tt>wcstombs</tt> and so on in advance.  The advantages of this
approach are:
<list>
  <item>The programmer don't need to know the detail of 
        international and local character codes.
</list>
However, there are a few disadvantages:
<list>
  <item>C functions related to wide / multibyte characters are
        bound to LC_CTYPE locale category.  This means these functions
        cannot handle two languages at the same time.  Thus 
        M17N is never achieved.
  <item>The software will not work well on OSes with poor locale
        support.
</list>
</P>

<P>
Since you may not assume anything about
implementation of wide character (value of <tt>wchar_t</tt>),
you cannot do anything more than the library prepares,
for example, obtain number of columns a character occupies.
</P>

<sect id="internal-length"><heading>Length of String</heading>

<P>
The following is a sample program to obtain the length of the
inputed string.  Note that number of bytes and number of characters
are not distinguished.
<example>
/* length.c
 *
 * a sample program to obtain the length of the inputed string
 * NOT INTERNATIONALIZED
 */

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

int main(int argc, char **argv)
{
	int len;

	if (argc &lt; 2) {
		printf("Usage: %s [string]\n", argv[0]);
		return 0;
	}
	
	printf("Your string is: \"%s\".\n", argv[1]);

	len = strlen(argv[1]);
	printf("Length of your string is: %d bytes.\n", len);
	printf("Length of your string is: %d characters.\n", len);
	return 0;
}
</example>
</P>

<P>
The following is a internationalized version of the program
using wide characters.
<example>
/* length-i.c
 *
 * a sample program to obtain the length of the inputed string
 * INTERNATIONALIZED
 */

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;locale.h&gt;

int main(int argc, char **argv)
{
	int len, n;
	wchar_t *wp;

	/* All softwares using locale should write this line */
	setlocale(LC_ALL, "");

	if (argc &lt; 2) {
		printf("Usage: %s [string]\n", argv[0]);
		return 0;
	}
	
	printf("Your string is: \"%s\".\n", argv[1]);

	/* The concept of 'byte' is universal. */
	len = strlen(argv[1]);
	printf("Length of your string is: %d bytes.\n", len);

	/* To obtain number of characters, it is the easiest way */
	/* to convert the string into wide string.  The number of */
	/* characters is equal to the number of wide characters. */
	/* It does not exceed the number of bytes. */
	n = strlen(argv[1]) * sizeof(wchar_t);
	wp = (wchar_t *)malloc(n);
	len = mbstowcs(wp, argv[1], n);
	printf("Length of your string is: %d characters\n", len);

	return 0;
}
</example>
</P>

<P>
This program can count multibyte characters correctly.
Of course the user has to set LANG variable properly.
</P>

<sect id="internal-extract"><heading>Extraction of Characters</heading>

<P>
The following program extracts all characters contained in the given
string.
<example>
/* extract.c
 *
 * a sample program to extract each character contained in the string
 * not internationalized
 */

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

int main(int argc, char **argv)
{
	char *p;
	int c;

	if (argc &lt; 2) {
		printf("Usage: %s [string]\n", argv[0]);
		return 0;
	}
	
	printf("Your string is: \"%s\".\n", argv[1]);

	c = 0;
	for (p=argv[1] ; *p ; p++) {
		printf("Character #%d is \"%c\".\n", ++c, *p);
	}
	return 0;
}
</example>
</P>

<P>
Using wide characters, the program can be rewritten as following.
<example>
/* extract-i.c
 *
 * a sample program to extract each character contained in the string
 * INTERNATIONALIZED
 */

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;locale.h&gt;
#include &lt;stdlib.h&gt;

int main(int argc, char **argv)
{
	wchar_t *wp;
	char p[MB_CUR_MAX+1];
	int c, n, len;

	/* Don't forget. */
	setlocale(LC_ALL, "");

	if (argc &lt; 2) {
		printf("Usage: %s [string]\n", argv[0]);
		return 0;
	}
	
	printf("Your string is: \"%s\".\n", argv[1]);

	/* To obtain each character of the string, it is easy to convert */
	/* the string into wide string and re-convert each of the wide */
	/* string into multibyte characters. */
	n = strlen(argv[1]) * sizeof(wchar_t);
	wp = (wchar_t *)malloc(n);
	len = mbstowcs(wp, argv[1], n);
	for (c=0; c&lt;len; c++) {
		/* re-convert from wide character to multibyte character */
		int x;
		x = wctomb(p, wp[c]);
		/* One multibyte character may be two or more bytes. */
		/* Thus "%s" is used instead of "%c". */
		if (x&gt;0) p[x]=0;
		printf("Character #%d is \"%s\" (%d byte(s)) \n", c, p, x);
	}

	return 0;
}
</example>
</P>

<P>
Note that this program doesn't work well if the multibyte character
is stateful.
</P>

<chapt id="other"><heading>Other Special Topics</heading>

<sect id="locale-"><heading>Locale in C</heading>

<P>
Locale is the main faculty for I18N of C language.
</P>

<P>
Locale model is that a software changes its behavior
according to its language environment.  The environment can be
set independently for six categories of
LC_COLLATE, LC_CTYPE, LC_MESSAGES, LC_MONETARY, LC_NUMERIC,
and LC_TIME.
C library supplies a set of functions which changes their
behaviors according to one of the six locale categories.
To internationalize a software, use these functions.
Don't forget to call <tt>setlocale</tt> function at first
or these functions would not change their behavior.
</P>

<P>
If <tt>setlocale(LC_ALL, "")</tt> is described at the start of the
software, the choice of the environment is done by environmental variables
whose names are same to the names of categories.
If LC_ALL variable is defined, LC_ALL takes precedence over these
variables.  If neither of them are defined, LANG variable is adopted.
If LANG is also not defined, 'C' locale, which means default behavior,
is used.
</P>

<P>
Though valid values for these environmental variables (locale names) 
depend on the kind and set-up of the OS, the format of locale names
is usually like <tt>ja_JP.eucJP</tt>, where two lowercase characters
mean language (<tt>ja</tt> = Japanese), two capital characters 
mean country (<tt>JP</tt> = Japan), and characters after dot mean 
character code (<tt>eucJP</tt> = EUC-JP).  Type <tt>locale -a</tt> to 
display all valid locale names.  However, it is users' responsibility to
set proper value to LANG variable and the developers don't need to
be aware of the value of the LANG variable.
</P>

<P>
Many people tend to think that I18N means <tt>gettext</tt>ization 
and translation of messages.  However, it is mere one category
(LC_MESSAGES) out of six categories.  The most important category
is LC_CTYPE.
</P>

<P>
However, note that M17N is not achieved by locale mechanism,
especially LC_CTYPE.  You have to use international
character codes such as ISO 2022 and Unicode instead of LC_CTYPE mechanism
to write M17N-ed software.  Moreover, LC_CTYPE mechanism is
sometimes insufficient even for I18N.  For example,
<package>jless</package> is a text file viewer which can
automatically distinguishes three Japanese character codes and converts
into desirable character codes.  You cannot write such a software using
LC_CTYPE mechanism.
</P>

<sect id="wchar-"><heading>Multibyte and Wide characters in C</heading>

<P>
Standard C library supplies functions to handle multibyte and
wide characters.  These functions are sensible to LC_CTYPE
locale category.
</P>

<P>
Multibyte character is a character code which is used for <em>real</em>
input/output.  In other words, <em>the character code you usually use</em>
is the multibyte character whatever language you speak.
If you use ISO-8859-1, it is your multibyte character.
If you use EUC-KR, it is your multibyte character.
Despite the name, multibyte character may or may not be
expressed in multiple bytes.
</P>

<P>
Since multibyte character can be stateful (that is, can have 
shift status) and the number of bytes a character does not
have to be a constant, implementation using multibyte character
can be difficult.  For example, it may be difficult even to count
the number of characters.  Thus wide character can be used.
</P>

<P>
Wide character is a character code supplied by the standard C library
for easy handling of international strings.
Wide character is stateless and the size of every wide characters
are same.  Functions for conversion between multibyte character
and wide character (and string of multibyte characters and
string of wide characters) are supplied by library.
Wide character is expressed using <tt>wchar_t</tt> type.
String of wide characters is expressed
as a array of <tt>wchar_t</tt>, like string of ASCII characters is expressed
as a array of <tt>char</tt>.
</P>

<P>
Thus it is convenient to input multibyte characters from a stream,
convert them into wide characters, process, convert back into
multibyte characters, and output them to a stream.  <tt>wchar_t</tt> is 
used as an internal code.
</P>

<P>
Functions for conversion between multibyte and wide characters/strings 
are shown below:
<list>
 <item><tt>mbtowc()</tt> and <tt>mbrtowc()</tt> to convert 
       from multibyte to wide character.
 <item><tt>mblen()</tt>, <tt>mbrlen()</tt> to obtain the number 
       of characters of multibyte character string.
 <item><tt>mbstowcs()</tt>, <tt>mbsrtowcs()</tt> to convert from 
       multibyte to wide character string.
 <item><tt>wctomb()</tt>, <tt>wcrtomb()</tt> to convert from wide 
       to multibyte character.
 <item><tt>wcstombs()</tt>, <tt>wcsrtombs()</tt> to convert from 
       wide to multibyte character string.
 <item><tt>mbsinit()</tt> to check shift status.
 <item><tt>btowc()</tt> and <tt>wctob()</tt> to convert 1byte and 
       wide characters.
</list>
</P>

<P>
'<tt>r</tt>' version of these functions (for example, <tt>mbrtowc</tt>) 
have an additional parameter to a pointer to a <tt>mbstate_t</tt> 
variable which contains the shift status.  Since non-'<tt>r</tt>' 
version of these functions have shift status in their internal 
(static) variable, these can treat only one succession of string at a time.
</P>

<P>
See manpages of these functions for further information.
</P>

<P>
The implementation of wchar_t is not determined by any 
standards, though UCS-4 is used for glibc.  You must not 
assume the implementation of <tt>wchar_t</tt>.
</P>

<P>
Though usual functions such as <tt>printf()</tt> can be used for multibyte
characters for input/output, one have to take care of escape 
character '<tt>%</tt>' used in formatted input/output functions, because 
a part of a multibyte character can have same value as ASCII 
code of '<tt>%</tt>'.  
</P>


<sect id="gettext"><heading>Gettext</heading>

<P>
Gettext is a tool to internationalize messages a software outputs
according to locale status of <tt>LC_MESSAGES</tt>.
A <prgn>gettext</prgn>ized software contains messages written in
various languages (according to available translators) and 
a user can choose them using environmental variables.
GNU gettext is a part of Debian system.
</P>

<P>
Install <package>gettext</package> package and read info pages for details.
</P>

<P>
Don't use non-ASCII characters for '<tt>msgid</tt>'.
Be careful because you may tend to use ISO-8859-1 characters.
For example, '&copy;' (copyright mark; you may be not able to
read the copyright mark NOW in THIS document) is non-ASCII character
(0xa9 in ISO-8859-1).
Otherwise, translators may feel difficulty to edit catalog files
because of conflict between character codes for <tt>msgid</tt> and in
<tt>msgstr</tt>.
</P>

<P>
Be sure the message can be displayed in the assumed environment.
In other words, you have to read the chapter of 'Output to Display' 
in this document and internationalize the output mechanism
of your software prior to <prgn>gettext</prgn>ization.
<em>ENGLISH MESSAGES ARE PREFERRED EVEN FOR NON-ENGLISH-SPEAKING PEOPLE,
THAN MEANINGLESS BROKEN MESSAGES.</em>
</P>

<P>
The 2nd (3rd, ...) byte of multibyte characters or 
all bytes of non-ASCII characters in stateful character codes
can be 0x5c (same to backslash in ASCII) or 0x22
(same to double quote in ASCII).
These characters have to properly escaped because
present version of GNU gettext doesn't care the 
'charset' subitem of '<tt>Content-Type</tt>' item for '<tt>msgstr</tt>'.
</P>

<P>
A <prgn>gettext</prgn>ed message must not used in multiple contexts.
This is because a word may have different meaning in different context.
For example, a verb means an order or a command if it appears
at the top of the sentence in English.  However, different languages
have different grammar.  If a verb is <prgn>gettext</prgn>ed and it is used
both in a usual sentence and in an imperative sentence,
one cannot translate it.
</P>


<P>
If a sentence is <prgn>gettext</prgn>ed, never divide the sentence.
If a sentence is divided in the original source code,
connect them so as to single string contains the full
sentence.  
This is because the order of words in a sentence
is different among languages.
For example, a routine
<example>
printf("There ");
switch(num_of_files) {
case 0:
        printf("are no files ");
        break;
case 1:
        printf("is 1 file ");
        break;
default:
        printf("are %d files ", num_of_files);
        break;
}
printf("in %s directory.\n", dir_name);
</example>
has to be written like that:
<example>
switch(num_of_files) {
case 0:
        printf("There are no files in %s directory", dir_name);
        break;
case 1:
        printf("There is 1 file in %s directory", dir_name);
        break;
default:
        printf("There are %d files in %s directory", num_of_files, dir_name);
        break;
}
</example>
before it is <prgn>gettext</prgn>ized.
</P>

<P>
A software with <prgn>gettext</prgn>ed messages should not depend on
the length of the messages.  The messages may get longer
in different language.
</P>

<P>
When two or more '%' directive for formatted output functions
such as <tt>printf()</tt> appear in a message,
the order of these '%' directives may be changed by
translation.  In such a case, the translator can specify
the order.
See section of 'Special Comments preceding Keywords'
in info page of <prgn>gettext</prgn> for detail.
</P>

<P>
Now there are projects to translate messages in various softwares.
For example, 
<url id="http://www.iro.umontreal.ca/~pinard/po/HTML/" 
name="Translation Project">.
</P>



<sect1 id="gettextize"><heading>Gettext-ize a software</heading>

<P>
At first, the software has to have the following lines.
<example>
int main(int argc, char **argv)
{
        ...
        setlocale (LC_ALL, "");   /* This is not for gettext but 
                                     all i18n software should have
                                     this line. */
        bindtextdomain (PACKAGE, LOCALEDIR);
        textdomain (PACKAGE);
        ...
}
</example>
where <var>PACKAGE</var> is the name of the catalog file and 
<var>LOCALEDIR</var> is <tt>"/usr/share/locale"</tt> for Debian.
<var>PACKAGE</var> and <var>LOCALEDIR</var> should be defined 
in a header file or <tt>Makefile</tt>.
</P>

<P>
It is convenient to prepare the following header file.
<example>
#include &lt;libintl.h&gt;
#define _(String) gettext((String))
</example>
and messages in source files should be written as
<tt>_("message")</tt>, instead of <tt>"message"</tt>.
</P>

<P>
Next, catalog files have to be prepared.
</P>

<P>
At first, a template for catalog file is prepared
using <prgn>xgettext</prgn>.
At default a template file <tt>message.po</tt> is
prepared.
<footnote>
I HAVE TO WRITE EXPLANATION.
</footnote>
</P>



<sect1 id="gettext-translate"><heading>Translation</heading>

<P>
Though <prgn>gettext</prgn>ization of a software is a temporal
work, translation is a continuing work because you have to 
translate new (or modified) messages when (or before) a new 
version of the software is released.
</P>

<sect id="mailnews"><heading>Mail/News</heading>

<P>
Internet mail uses SMTP (RFC 821) and ESMTP (RFC 1869) protocols.
SMTP is 7bit protocol and ESMTP is 8bit.
</P>

<P>
Original SMTP can only send ASCII characters.  Thus 
non-ASCII characters (ISO 8859-*, Asian characters, and so on)
have to be converted into ASCII characters.
</P>

<P>
MIME (RFC 2045, 2046, 2047, 2048, and 2049) deals with this problem.
</P>

<P>
At first RFC 2045 determines three new headers.
<list>
 <item>MIME-Version:
 <item>Content-Type:
 <item>Content-Transfer-Encoding:
</list>
Now <tt>MIME-Version</tt> is 1.0 and thus all MIME mails have
a header like this:
<example>
MIME-Version: 1.0
</example>
<tt>Content-Type</tt> describes the type of content.
For example, an usual mail with Japanese text has a header like that:
<example>
Content-Type: text/plain; charset="iso-2022-jp"
</example>
Available types are described in RFC 2046.
<tt>Content-Transfer-Encoding</tt> describes the way to
convert the contents. Available values are <tt>BINARY</tt>,
<tt>7bit</tt>, <tt>8bit</tt>, <tt>BASE64</tt>, and <tt>QUOTED-PRINTABLE</tt>.
Since SMTP cannot handle 8bit data, <tt>8bit</tt> and <tt>BINARY</tt>
cannot be used.  ESMTP can use them.
Base64 and quoted-printable are ways to convert 8bit data into 7bit
and 8bit data have to be converted using either of them to sent by SMTP.
</P>

<P>
RFC 2046 describes media type and sub type for 
<tt>Content-Type</tt> header. Available types are
<tt>text</tt>, <tt>image</tt>, <tt>audio</tt>, <tt>video</tt>,
and <tt>application</tt>.  Now we are interested in <tt>text</tt>
because we are discussing about i18n.
Sub types for <tt>text</tt> are <tt>plain</tt>, <tt>enriched</tt>,
<tt>html</tt>, and so on.  <tt>charset</tt> parameter can also be
added to specify character codes.
<tt>US-ASCII</tt>, <tt>ISO-8859-1</tt>, 
<tt>ISO-8859-2</tt>, ..., <tt>ISO-8859-10</tt> are defined by
RFC 2046 for <tt>charset</tt>.  This list can be added by writing
a new RFC.
<list>
 <item>RFC 1468 <tt>ISO-2022-JP</tt>
 <item>RFC 1554 <tt>ISO-2022-JP-2</tt> 
 <item>RFC 1557 <tt>ISO-2022-KR</tt>
 <item>RFC 1922 <tt>ISO-2022-CN</tt>
 <item>RFC 1922 <tt>ISO-2022-CN-EXT</tt>
 <item>RFC 1842 <tt>HZ-GB-2312</tt>
 <item>RFC 1641 <tt>UNICODE-1-1</tt>
 <item>RFC 1642 <tt>UNICODE-1-1-UTF-7</tt>
 <item>RFC 1815 <tt>ISO-10646-1</tt>
</list>
</P>

<P>
RFC 2045 and 2046 determine the way to write non-ASCII characters
in the main text of mail.  On the other hand, RFC 2047 describes
'encoded words' which is the way to write non-ASCII characters in the header.
It is like that:
<tt>=?</tt><var>character code</var><tt>?</tt><var>conversion algorithm</var><tt>?</tt><var>data</var><tt>?=</tt>,
where <var>character code</var> is selected from the list of <tt>charset</tt>
of <tt>Content-Type</tt> header, <var>algorithm</var> is <tt>Q</tt>
or <tt>q</tt> for quoted-printable or <tt>B</tt> or <tt>b</tt> for
base64, and <var>data</var> is encoded data whose length is less than
76 bytes.  If the <var>data</var> is longer than 75 bytes, 
it must be divided into multiple encoded words.
For example,
<example>
Subject: =?ISO-2022-JP?B?GyRCNEE7eiROJTUlViU4JSclLyVIGyhC?=
</example>
reads 'a subject written in Kanji' in Japanese (ISO-2022-JP,
encoded by base64).  Of course human cannot read it.
</P>


<sect id="www"><heading>WWW</heading>

<P>
WWW is a system that HTML documents (mainly; and files in other formats) 
are transferred using HTTP protocol.
</P>

<P>
HTTP protocol is defined by RFC 2068.
HTTP uses headers like mails and <tt>Content-Type</tt> header
is used to describe the type of the contents.
Though <tt>charset</tt> parameter can be described in the
header, it is rarely used.
</P>

<P>
RFC 1866 describes that the default character code for HTML is
ISO-8859-1.  However, many web pages are written in,
for example, Japanese and Korean using (of course) character codes
different from ISO-8859-1.
Sometimes the HTML document describes:
<example>
&lt;META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-2022=jp"&gt;
</example>
which declares that the page is written in ISO-2022-JP.
However, there many pages without any declaration of character code.
</P>

<P>
Web browsers have to deal with such a circumstance.
Of course web browsers have to be able to deal with every
character codes in the world which is listed in MIME.
However, many web browsers can only deal with ASCII
or ISO-8859-1.  Such web browsers are useless at all
for non-ASCII or non-ISO-8859-1 people.
</P>

<P>
URL should be written in ASCII character,
though non-ASCII characters can be expressed
using <tt>%</tt><var>nn</var> sequence where <var>nn</var>
is hexadegimal value.  This is because there are
no way to specify character code. Wester-European people
would treat it as ISO-8859-1, while Japanese people
would treat it as EUC-JP or SHIFT-JIS.
</P>


<sect id="filename"><heading>File Names</heading>

<P>
I heard that Linux has a potential to deal with filenames written in 
UTF-8.  Is it true?
</P>


<chapt id="examples"><heading>Examples of I18N</heading>

<P>
Programmers who have internationalized softwares, have
written a patch of L10N, and so on are encouraged to contribute
to this chapter.
</P>



&minicom;
&user-ja;
&fontset;








<chapt id="reference"><heading>References</heading>

<P>
General
<list>
 <item>
   <url id="http://i44www.info.uni-karlsruhe.de/~drepper/conf96/paper.html" 
   name="i18n in GNU Project">
 <item>
   <url id="http://cns-web.bu.edu/pub/djohnson/web_files/i18n/i18n.html" 
   name="Concept of C/UNIX i18n">
</list>
</P>

<P>
Characters (general)
<list>
 <item>
   <url id="http://www.kudpc.kyoto-u.ac.jp/~yasuoka/CJK.html"
   name="Character Tables">
   Graphic images for various character sets in the world.
 <item>
   <url id="ftp://ftp.ora.com/pub/examples/nutshell/ujip/doc/cjk.inf"
   name="Ken Lunde's CJK info">
   information on CJK (Chinese, Japanese, and Korean) character 
   set standards, written by the writer of "CJKV Information Processing"
   published by O'Reilly.
</list>
</P>

<P>
Characters (ISO 8859)
<list>
 <item>
   <url id="http://czyborra.com/charsets/iso8859.html">
 <item>
   <url id="http://park.kiev.ua/multiling/ml-docs/iso-8859.html">
 <item>
   <url id="http://www.terena.nl/projects/multiling/ml-docs/iso-8859.html">
</list>
</P>

<P>
Characters (ISO 2022)
<list>
 <item>
   <url id="http://www.ewos.be/tg-cs/gconcept.htm">
 <item>
   <url id="http://www.ecma.ch/stand/ECMA-035.HTM">
</list>
</P>

<P>
Characters (Unicode)
<list>
 <item><url id="http://www.unicode.org/">
</list>
</P>

<P>
Example of i18n
<list>
 <item>
   <url id="http://www.wg.omron.co.jp/~shin/Arena-CJK-doc/"
   name="Arena-i18n">
   Multilingual web browser.
 <item>
   <url id="http://www.m17n.org/mule/" name="Mule">
   Multilingual editor whose function is included in GNU Emacs 20
   and XEmacs 20.
   Mule is the most advanced m17n software in my knowledge.
</list>
</P>

<P>
Projects
<list>
 <item>
   <url id="http://www.li18nux.org/" 
   name="Linux Internationalization Initiative">, or Li18nux,
   focuses on the i18n of a core set of APIs and components of Linux
   distributions.  The results will be proposed to LSB.
 <item>
   <url id="http://www.iro.umontreal.ca/~pinard/po/HTML/" 
   name="Translation Project">
 <item>
   <url id="http://www.zepler.org/~rwb197/xterm/"
   name="XTerm with Doublewidth/Combining Characters">
   Though XTerm in XFree86 4.0 supports Unicode, it does not
   supports doublewidth nor combining characters.  This is a 
   project to add support for these characters.
</list>
<P>





</book>
</debiandoc>
